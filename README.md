# RAG Project using Cohere API and Sentence Transformers

## Description

This project implements a Retrieval-Augmented Generation (RAG) model using the Cohere API for large language model (LLM) responses, `sentence-transformers/all-MiniLM-L6-v2` for word embeddings and https://www.gutenberg.org/cache/epub/24022/pg24022.txt for document. The system retrieves the top 5 most relevant documents based on the embedding model and generates a response using Cohere's API.You can ask something about above website.

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Testing LLM and Embedding Models Separately](#testing-llm-and-embedding-models-separately)
- [Features](#features)
- [API Key Setup](#api-key-setup)
- [Contributing](#contributing)

## Installation

### Prerequisites

- Python 3.9 or higher
- Poetry (for dependency management)

### Steps

1. Clone the repository:
   ```bash
   git clone https://github.com/Sean12344321/RAG-project.git
   cd RAG-project
   ```
2. Install dependencies using Poetry:
   ```bash
   poetry install
   ```

## Usage

Once the dependencies are installed, follow these steps:

1. Navigate to the `src` directory:

   ```bash
   cd rag/src
   ```

2. Run the generator module:

   ```bash
   python -m generator
   ```

3. After the program starts, you can ask a question, and the system will output:
   - The top 5 relevant documents retrieved by the embedding model.
   - A response generated by Cohere's API based on the question.

## Testing LLM and Embedding Models Separately

You can also test the models individually:

1. **Testing the LLM model:**

   - Navigate to the `tests` directory:
     ```bash
     cd tests
     ```
   - Run the LLM model:
     ```bash
     python -m llm_model
     ```
   - You can then ask Cohere a question, and it will generate a response.

2. **Testing the embedding model:**
   - Run the embedding model:
     ```bash
     python -m embedding_model
     ```
   - You can enter an input, and the output will be the similarity score across the documents.

## Features

- Retrieval-Augmented Generation combining word embedding and LLM.
- Top 5 relevant documents are retrieved using `sentence-transformers/all-MiniLM-L6-v2`.
- Cohere's API generates responses based on the retrieved context.
- Separate testing for LLM and embedding models.

## Setup API Key

To use this project, you'll need a Cohere API key. Follow the steps below to generate and set up your API key.

1. Go to [Cohere Dashboard](https://dashboard.cohere.com/api-keys) and sign up for a free account (if you don't have one).
2. Generate a new API key from the dashboard.
3. Once you have the API key, you have two options:

### Option 1: Using `.env` file (Recommended)

1. Create a `.env` file in the root of the project.
2. Add your API key in the following format:

   ```bash
   COHERE_API_KEY=your-api-key-here
   ```

3. Ensure the code in `generator.py` uses the following line to load the API key from the environment:

   ```python
   qa_model = cohere.Client(os.getenv('COHERE_API_KEY'))
   ```

### Option 2: Hardcode the API key (Not Recommended)

If you prefer not to use a `.env` file, you can directly replace the code in `generator.py`:

1. Open the `generator.py` file.
2. Find the following line of code:

   ```python
   qa_model = cohere.Client(os.getenv('COHERE_API_KEY'))
   ```

3. Replace it with:

   ```python
   qa_model = cohere.Client("your-api-key-here")
   ```

## Contributing

Contributions to this project are welcome! If you'd like to contribute, follow these steps:

1. Fork the repository.
2. Create a new branch for your feature or bug fix:
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. Make your changes and commit them:
   ```bash
   git commit -m "Add some feature"
   ```
4. Push to your branch:
   ```bash
   git push origin feature/your-feature-name
   ```
5. Open a pull request on the original repository.
