# RAG Project using gemma2:2b and Sentence Transformers

## Description

This project implements a Retrieval-Augmented Generation (RAG) model using the gemma2:2b for large language model (LLM) responses, `sentence-transformers/all-MiniLM-L6-v2` for word embeddings and https://www.gutenberg.org/cache/epub/24022/pg24022.txt for document. The system retrieves the top 5 most relevant documents based on the embedding model and generates a response using gemma2:2b.You can ask something about above website.

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Testing LLM and Embedding Models Separately](#testing-llm-and-embedding-models-separately)
- [Features](#features)
- [Contributing](#contributing)

## Installation

### Prerequisites

- Python 3.9 or higher
- Poetry (for dependency management)

### Steps

1. Clone the repository:
   ```bash
   git clone https://github.com/Sean12344321/RAG-project.git
   cd RAG-project
   ```
2. Install dependencies using Poetry:
   ```bash
   poetry install
   ```

## Usage

Once the dependencies are installed, follow these steps:

1. Navigate to the `src` directory:

   ```bash
   cd rag/src
   ```

2. Run the generator module:

   ```bash
   python -m generator
   ```

3. After the program starts, you can ask a question, and the system will output:
   - The top 5 relevant documents retrieved by the embedding model.
   - A response generated by gemma2:2b based on the question.

## Testing LLM and Embedding Models Separately

You can also test the models individually:

1. **Testing the LLM model:**

   - Navigate to the `tests` directory:
     ```bash
     cd tests
     ```
   - Run the LLM model:
     ```bash
     python -m llm_model
     ```
   - You can then ask Cohere a question, and it will generate a response.

2. **Testing the embedding model:**
   - Run the embedding model:
     ```bash
     python -m embedding_model
     ```
   - You can enter an input, and the output will be the similarity score across the documents.

## Features

- Retrieval-Augmented Generation combining word embedding and LLM.
- Top 5 relevant documents are retrieved using `sentence-transformers/all-MiniLM-L6-v2`.
- gemma2:2b generates responses based on the retrieved context.
- Separate testing for LLM and embedding models.

## Contributing

Contributions to this project are welcome! If you'd like to contribute, follow these steps:

1. Fork the repository.
2. Create a new branch for your feature or bug fix:
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. Make your changes and commit them:
   ```bash
   git commit -m "Add some feature"
   ```
4. Push to your branch:
   ```bash
   git push origin feature/your-feature-name
   ```
5. Open a pull request on the original repository.
